{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "508ab4a1-d4f2-4db2-8873-fad89284997e",
   "metadata": {},
   "source": [
    "## Steps prior to run the experiments\n",
    "1. Download all files, they are in `tar.gz` format, decompress those.\n",
    "2. Create two new directories, one for all files from 2021 and one for 2022.\n",
    "3. Each new directory should contain four sub-directories, output-naive-accurate, output-naive-rpcid, output-partial, output-rebuild.\n",
    "All experiments run on one year's data.\n",
    "4. Each sub-directory contain trace file(s) in `csv.gz` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b775c5bf-ec76-47e4-9b01-c234e29adbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to downloaded data. the top level directory should contain four sub-directories, output-naive-accurate, output-naive-rpcid, output-partial, output-rebuild\n",
    "path = 'SET_YOUR_PATH' \n",
    "year = '2021' # should be '2021' or '2022'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b131f2be-3fa7-44a2-9919-a73c90a22eab",
   "metadata": {},
   "source": [
    "## Required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b27c9-2afa-4d1c-8c9c-a4d9c3b57189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required pacakges\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f3a065-7109-4e28-bcbb-e788b54e801a",
   "metadata": {},
   "source": [
    "## Compare three topological characteristics of output traces\n",
    "Maximum depth, maximum fanout, and size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6280c98d-29d2-4dc9-bd54-14ea1514c305",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModeCharacterizer:\n",
    "    \n",
    "    def __init__(self, path, the_year, mode):\n",
    "        self.num_traces_processed = 0\n",
    "        self.dir = path\n",
    "        self.year = the_year\n",
    "        self.size_list = []\n",
    "        self.depth_list = []\n",
    "        self.width_list = []\n",
    "        self.max_fanout_list = []\n",
    "        self.mode = mode\n",
    "        \n",
    "\n",
    "    def process_all_files(self):\n",
    "        directory_path = self.dir\n",
    "        num_files = 0 \n",
    "        \n",
    "        for root, dirs, files in os.walk(directory_path):\n",
    "            \n",
    "            for file in files: # all files has .csv extensions\n",
    "                if file.endswith('.csv.gz'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    num_files += 1\n",
    "                    \n",
    "                    # Read the CSV file into a DataFrame\n",
    "                    # Specify the first 9 column names or indices\n",
    "                    columns_to_read = []\n",
    "\n",
    "                    if self.year == '2021':\n",
    "                        columns_to_read = ['traceid', 'timestamp', 'rpcid', 'um', 'rpctype', 'dm', 'interface', 'rt']\n",
    "                    else:\n",
    "                        columns_to_read = ['traceid', 'rpcid', 'rpctype','um', 'interface', 'dm',  'rt']\n",
    "                    \n",
    "                    df = pd.read_csv(file_path, compression='gzip', usecols=columns_to_read)\n",
    "                    df['rpcid'] = df['rpcid'].astype(str)\n",
    "                    self.process_one_file(df)\n",
    "                    \n",
    "                    del df\n",
    "\n",
    "    # used to sort rpcids\n",
    "    def key(self, rpcid):\n",
    "        return rpcid.count('.')\n",
    "\n",
    "    def sort_rpcids(self, rpcids):\n",
    "        # sort first by # of periods then by last digit \n",
    "        return sorted(rpcids, key=self.key)\n",
    "        \n",
    "    def process_one_file(self, df):\n",
    "        trace_dfs= df.groupby('traceid')\n",
    "        \n",
    "        for tid, trace_df in trace_dfs:\n",
    "            rpcids = self.sort_rpcids(trace_df['rpcid'].tolist())\n",
    "            rpcid_to_root = self.find_all_roots(rpcids)\n",
    "            root_to_rpcids = self.get_trees(rpcid_to_root)\n",
    "\n",
    "            cur_size = self.calculate_size(root_to_rpcids)\n",
    "            self.size_list.append(cur_size)\n",
    "\n",
    "            depth = self.calculate_max_depth(root_to_rpcids)\n",
    "            roots = list(root_to_rpcids.keys())\n",
    "\n",
    "    \n",
    "            width = self.calculate_max_fanout(roots, rpcids)\n",
    "            self.depth_list.append(depth)\n",
    "            self.width_list.append(width)\n",
    "            self.num_traces_processed += 1\n",
    "\n",
    "    # the number of fanout is the number of children the parent node calls in a trace\n",
    "    def calculate_max_fanout(self, roots, rpcids):\n",
    "        parent_to_numChildern = {}\n",
    "\n",
    "        for rpcid in rpcids:\n",
    "            \n",
    "            if rpcid in roots:\n",
    "                continue\n",
    "            parent = rpcid.rsplit('.', 1)[0]\n",
    "            if parent not in parent_to_numChildern:\n",
    "                parent_to_numChildern[parent] = 1\n",
    "            else:\n",
    "                parent_to_numChildern[parent] += 1\n",
    "\n",
    "        if len(parent_to_numChildern) == 0:\n",
    "            return 0\n",
    "        return max(parent_to_numChildern.values())           \n",
    "        \n",
    "\n",
    "    # return all subtress in a call graph,\n",
    "    # a dictionary storing the map from each root rpcid and all its decedent \n",
    "    def get_trees(self, rpcid_to_root):\n",
    "        root_to_rpcids = {}\n",
    "\n",
    "        for key, value in rpcid_to_root.items():\n",
    "            if value in root_to_rpcids:\n",
    "                root_to_rpcids[value].append(key)\n",
    "            else:\n",
    "                root_to_rpcids[value] = [key]\n",
    "            \n",
    "        return root_to_rpcids\n",
    "\n",
    "    # the size of a trace is the number of nodes in the call graph\n",
    "    # the same microservce could repeat in a call graph, and each occurrence adds 1 to the size\n",
    "    def calculate_size(self,root_to_rpcids):\n",
    "        size = 0\n",
    "        for root in root_to_rpcids:\n",
    "            size += (len(root_to_rpcids[root]) + 1)\n",
    "        return size\n",
    "\n",
    "    # root is of depth 1\n",
    "    def calculate_max_depth(self, root_to_rpcids):\n",
    "        depth_to_numNode = {1: 0}\n",
    "\n",
    "        for root in root_to_rpcids:\n",
    "            rpcids = root_to_rpcids[root]\n",
    "            #handle root\n",
    "            depth_to_numNode[1] += 1\n",
    "            \n",
    "            for rpcid in rpcids:\n",
    "                depth = rpcid.count('.') - root.count('.') + 2\n",
    "                \n",
    "                if depth not in depth_to_numNode:\n",
    "                    depth_to_numNode[depth] = 1\n",
    "                else:\n",
    "                    depth_to_numNode[depth] += 1\n",
    "\n",
    "        return max(depth_to_numNode)\n",
    "\n",
    "    # A root is a rpcid whose parent rpcid does not exist in the trace\n",
    "    def find_all_roots(self, rpcids):\n",
    "        rpcid_to_root = {}\n",
    "        \n",
    "        for rpcid in rpcids:\n",
    "            # handle special case for 2022\n",
    "            if rpcid == '0':\n",
    "                rpcid_to_root[rpcid] = rpcid\n",
    "                continue\n",
    "\n",
    "            # handle special case, root rpcid could be anything, e.g. 25\n",
    "            if rpcid.count('.') == 0:\n",
    "                rpcid_to_root[rpcid] = rpcid\n",
    "                continue \n",
    "\n",
    "            parent = rpcid.rsplit('.', 1)[0]\n",
    "            if parent not in rpcids:\n",
    "                rpcid_to_root[rpcid] = rpcid\n",
    "            else:\n",
    "                rpcid_to_root[rpcid] = rpcid_to_root[parent]\n",
    "                \n",
    "        return rpcid_to_root\n",
    "        \n",
    "    \n",
    "    def get_num_traces(self):\n",
    "        return self.num_traces_processed\n",
    "    \n",
    "    def get_trace_sizes(self):\n",
    "        return self.size_list\n",
    "\n",
    "    def get_trace_widths(self):\n",
    "        return self.width_list\n",
    "\n",
    "    def get_trace_depths(self):\n",
    "        return self.depth_list\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27831154-fb32-4810-b7c1-dcdedd15e278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a ModeCharacterizer for each of the four modes\n",
    "def init_trace_characterizer(path, year, mode):\n",
    "    trace_char = ModeCharacterizer(path, year, mode)\n",
    "    trace_char.process_all_files() \n",
    "    return trace_char\n",
    "\n",
    "# path for each mode\n",
    "naive_rpcid_path = path + 'output-naive-rpcid/'\n",
    "naive_accurate_path = path + 'output-naive-accurate/'\n",
    "partial_path = path + 'output-partial/'\n",
    "rebuild_path = path + 'output-rebuild/'\n",
    "\n",
    "naive_rpcid_char = init_trace_characterizer(naive_rpcid_path, year, 'naive_rpcid')\n",
    "naive_accurate_char = init_trace_characterizer(naive_accurate_path, year, 'naive_accurate')\n",
    "partial_char = init_trace_characterizer(partial_path, year, 'partial')\n",
    "rebuild_char = init_trace_characterizer(rebuild_path, year, 'rebuild')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87616fe-8e21-46f3-880c-feadbd62991b",
   "metadata": {},
   "source": [
    "## Plot the CDFs to compare trace sizes, maximum trace depth and maximum trace width for four different modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdd9e2d-df60-4e43-943b-79524052b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# figure style setup\n",
    "sns.set(font_scale=1.8)\n",
    "sns.set_style(\"whitegrid\")\n",
    "(width, legend_font) = (5, 25)\n",
    "(c1, c2, c3, c4) = (\"#d7191c\", \"#fdae61\", \"#abd9e9\", \"#2c7bb6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c301b87-07fa-4a11-8496-a22808d27dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare trace sizes\n",
    "sns.ecdfplot(data=naive_rpcid_char.get_trace_sizes(), label=\"Naive-rpcid\", color=c1, linewidth = 2)\n",
    "sns.ecdfplot(data=naive_accurate_char.get_trace_sizes(), label=\"Naive-accurate\", color=c2, linewidth = 2)\n",
    "sns.ecdfplot(data=partial_char.get_trace_sizes(), label=\"Partial\", color=c3, linewidth = 2)\n",
    "sns.ecdfplot(data=rebuild_char.get_trace_sizes(), label=\"Casper\", color=c4, linewidth = 2)\n",
    "\n",
    "plt.xlabel(\"Trace Size\")\n",
    "plt.xscale('log')\n",
    "plt.gca().set_ylabel('')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3885885-a06d-45d0-a892-45be3af0d0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare maximum depths\n",
    "sns.ecdfplot(data=naive_rpcid_char.get_trace_depths(), label=\"Naive-rpcid\", color=c1, linewidth = 2)\n",
    "sns.ecdfplot(data=naive_accurate_char.get_trace_depths(), label=\"Naive-accurate\", color=c2, linewidth = 2)\n",
    "sns.ecdfplot(data=partial_char.get_trace_depths(), label=\"Partial\", color=c3, linewidth = 2)\n",
    "sns.ecdfplot(data=rebuild_char.get_trace_depths(), label=\"Casper\", color=c4, linewidth = 2)\n",
    "\n",
    "plt.xlabel(\"Max Depth\")\n",
    "plt.gca().set_ylabel('')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a7117-da50-4a7b-8386-40dc8749b666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare maximum fanouts\n",
    "sns.ecdfplot(data=naive_rpcid_char.get_trace_widths(), label=\"Naive-rpcid\", color=c1, linewidth = 2)\n",
    "sns.ecdfplot(data=naive_accurate_char.get_trace_widths(), label=\"Naive-accurate\", color=c2, linewidth = 2)\n",
    "sns.ecdfplot(data=partial_char.get_trace_widths(), label=\"Partial\", color=c3, linewidth = 2)\n",
    "sns.ecdfplot(data=rebuild_char.get_trace_widths(), label=\"Casper\", color=c4, linewidth = 2)\n",
    "\n",
    "plt.xlabel(\"Max Width\")\n",
    "plt.gca().set_ylabel('')\n",
    "plt.xscale('log')\n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1028ec92-0d3a-4b80-ac4e-ba8a3e50ece1",
   "metadata": {},
   "source": [
    "## Impact of recovery mechanisms in CASPER algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23347c0d-2a6f-49a9-aa08-90a7596c0569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Classes for next part\n",
    "class FilledMissValCharacterizer:\n",
    "    \n",
    "    def __init__(self, homepath, rpcid_file, rebuild_file, the_year):\n",
    "        self.homepath = homepath\n",
    "        self.num_traces_processed = 0\n",
    "        self.rpcid_file = rpcid_file\n",
    "        self.rebuild_file = rebuild_file\n",
    "        self.tid_to_lists = {}  # tid is trace_id\n",
    "\n",
    "        self.output_lists = {'um_num' : [], \n",
    "                             'um_perc' : [],\n",
    "                             'dm_num' : [], \n",
    "                             'dm_perc' : [] }\n",
    "        \n",
    "        self.year = the_year\n",
    "\n",
    "\n",
    "    def process_file(self):\n",
    "        columns_to_read = []\n",
    "        if self.year == '2021':\n",
    "            columns_to_read = ['traceid', 'timestamp', 'rpcid', 'um', 'rpctype', 'dm', 'interface', 'rt']\n",
    "        else:\n",
    "            columns_to_read = ['traceid', 'rpcid', 'rpctype','um', 'interface', 'dm',  'rt']\n",
    "            \n",
    "        rpcid_df = pd.read_csv(self.rpcid_file, compression='gzip', usecols=columns_to_read)\n",
    "        rebuild_df = pd.read_csv(self.rebuild_file, compression='gzip', usecols=columns_to_read)\n",
    "        self.populate_lists(rpcid_df)\n",
    "        \n",
    "        trace_dfs= rebuild_df.groupby('traceid')\n",
    "        \n",
    "        for tid, df in trace_dfs:\n",
    "            um_rpcid_list = self.tid_to_lists[tid]['um_rpcid_list']\n",
    "            dm_rpcid_list = self.tid_to_lists[tid]['dm_rpcid_list']\n",
    "            \n",
    "            # process um_df\n",
    "            if len(um_rpcid_list) > 0:\n",
    "                um_df = df[df['rpcid'].isin(um_rpcid_list)]\n",
    "                if len(um_df) > 0:\n",
    "                    um_num = (um_df['um'] != '(?)').sum()\n",
    "                    um_ratio = int(um_num) / int(len(um_df))\n",
    "                    um_perc = round( um_ratio*100, 2)\n",
    "                    self.output_lists['um_num'].append(um_num)\n",
    "                    self.output_lists['um_perc'].append(um_perc)\n",
    "            # process dm_df\n",
    "            if len(dm_rpcid_list) > 0:\n",
    "                dm_df = df[df['rpcid'].isin(dm_rpcid_list)]\n",
    "                if len(dm_df) > 0:\n",
    "                    dm_num = (dm_df['dm'] != '(?)').sum()\n",
    "                    dm_ratio = int(dm_num) / int(len(dm_df)) \n",
    "                    dm_perc = round( dm_ratio*100, 2)\n",
    "                    self.output_lists['dm_num'].append(dm_num)\n",
    "                    self.output_lists['dm_perc'].append(dm_perc)\n",
    "\n",
    "            self.num_traces_processed += 1\n",
    "        \n",
    "        self.save_lists()\n",
    "        del rpcid_df, rebuild_df\n",
    "\n",
    "    # save intermediate data\n",
    "    def save_lists(self):\n",
    "        file_name = os.path.basename(self.rebuild_file)\n",
    "        \n",
    "        um_df = pd.DataFrame({'um_num': self.output_lists['um_num'], 'um_perc': self.output_lists['um_perc']})\n",
    "        um_df.to_csv(self.homepath + file_name.rsplit('.', 1)[0] + \"-um.csv\" , index=False)\n",
    "\n",
    "        dm_df = pd.DataFrame({'dm_num': self.output_lists['dm_num'], 'dm_perc': self.output_lists['dm_perc']})\n",
    "        dm_df.to_csv(self.homepath + file_name.rsplit('.', 1)[0] + \"-dm.csv\", index=False)\n",
    "    \n",
    "    \n",
    "    # use rpcid_df to create two lists for each tid\n",
    "    def populate_lists(self, df):\n",
    "        trace_dfs= df.groupby('traceid')\n",
    "        \n",
    "        for tid, trace_df in trace_dfs:\n",
    "            self.tid_to_lists[tid] = {}\n",
    "            # only keep the rows that have missing values\n",
    "            um_df = trace_df[trace_df['um'] == '(?)']   \n",
    "            dm_df = trace_df[trace_df['dm'] == '(?)']\n",
    "                \n",
    "            self.tid_to_lists[tid]['um_rpcid_list'] = um_df['rpcid'].tolist()\n",
    "            self.tid_to_lists[tid]['dm_rpcid_list'] = dm_df['rpcid'].tolist()\n",
    "\n",
    "\n",
    "# Helper class to synthesize intermediate data\n",
    "class FilledMissValHelper:\n",
    "    \n",
    "    def __init__(self, path, the_year, um_or_dm):\n",
    "        self.num_traces_processed = 0\n",
    "        self.dir = path\n",
    "        #self.trace_data_exps = {}\n",
    "        self.year = the_year\n",
    "        self.um_or_dm = um_or_dm\n",
    "        self.um_num = []\n",
    "        self.um_perc = []\n",
    "        self.dm_num = []\n",
    "        self.dm_perc = []\n",
    "        \n",
    "\n",
    "    def process_all_files(self):\n",
    "        directory_path = self.dir\n",
    "        num_files = 0 \n",
    "        \n",
    "        for root, dirs, files in os.walk(directory_path):\n",
    "            for file in files: # all files has .csv extensions\n",
    "                if self.um_or_dm =='um':\n",
    "                    \n",
    "                    if file.endswith('um.csv'):\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        num_files += 1\n",
    "\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        self.um_num.extend(df['um_num'].tolist())\n",
    "                        self.um_perc.extend(df['um_perc'].tolist())\n",
    "                        del df\n",
    "                        \n",
    "                else:\n",
    "                    if file.endswith('dm.csv'):\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        num_files += 1\n",
    "\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        self.dm_num.extend(df['dm_num'].tolist())\n",
    "                        self.dm_perc.extend(df['dm_perc'].tolist())\n",
    "                    \n",
    "                        del df\n",
    "        self.clean_intermediate_files()  # delete intermediate data, the csv files stored at top level directory \n",
    "\n",
    "        if self.um_or_dm =='dm':\n",
    "            return self.dm_num, self.dm_perc\n",
    "        else:\n",
    "            return self.um_num, self.um_perc\n",
    "    \n",
    "    def clean_intermediate_files(self):\n",
    "        # Create a pattern to match all '.txt' files\n",
    "        pattern = os.path.join(self.dir, '*.csv')\n",
    "\n",
    "        # List all files in the directory with .txt extension\n",
    "        intermediate_files = glob.glob(pattern)\n",
    "\n",
    "        # Loop through the files and delete them\n",
    "        for file in intermediate_files:\n",
    "            try:\n",
    "                os.remove(file)\n",
    "                #print(f\"Deleted: {file}\")\n",
    "            except OSError as e:\n",
    "                print(f\"Error: {file} : {e.strerror}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555ce8fe-88b6-44bb-93c8-11da2d7ab1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImpactCharacterizer:\n",
    "    \n",
    "    def __init__(self, path, the_year):\n",
    "        self.num_traces_processed = 0\n",
    "        self.dir = path\n",
    "        self.year = the_year\n",
    "\n",
    "\n",
    "        # Recovery mechanism #1 \n",
    "        # Adding missing calls\n",
    "        self.num_miss_rpcid_list = []\n",
    "        \n",
    "        # Recovery mechanism #2\n",
    "        # filled in missing DMs \n",
    "        self.num_filled_dm_list = []\n",
    "        self.perc_filled_dm_list = []\n",
    "\n",
    "\n",
    "        self.num_rpcids_at_source_list = []\n",
    "        self.num_rpcids_downstream_list = []\n",
    "        self.num_downstream_cpes_list = []\n",
    "        \n",
    "        # # Recovery mechanism #3\n",
    "        # # Updated rpcids at the source of a CPE\n",
    "        # self.num_source_new_rpcids_list = []\n",
    "        # self.perc_source_new_rpcids_list = []\n",
    "\n",
    "        # # Recovery mechanism #4\n",
    "        # # Recovered rpcids downstream from a CPE\n",
    "        # self.num_downstream_rpcids_list = []\n",
    "        # self.perc_downstream_rpcids_list = []\n",
    "\n",
    "        # Additional complete traces\n",
    "        self.num_unrecoverable_rpcid_list = []\n",
    "    \n",
    "    \n",
    "    def process_files_for_adding_mising_calls(self):\n",
    "        directory_path = self.dir + \"output-rebuild/error-stats/\"\n",
    "        all_files = os.listdir(directory_path)\n",
    "        error_files = [filename for filename in all_files if \"errors\" in filename]\n",
    "\n",
    "        # Process each error file\n",
    "        for error_file in error_files:\n",
    "            #print(error_file)\n",
    "            file_path = os.path.join(directory_path, error_file)\n",
    "\n",
    "            # Open and parse the JSON file\n",
    "            with open(file_path, 'r') as file:\n",
    "                json_data = json.load(file)\n",
    "                for key in json_data:\n",
    "                    self.num_miss_rpcid_list.append(json_data[key][\"METADATA\"][\"missing_rpcids\"])\n",
    "                    self.num_unrecoverable_rpcid_list.append(json_data[key][\"METADATA\"][\"unrecoverable_rpcids\"])\n",
    "        \n",
    "    # use helper classed defined above to get data by comparing the missing dms in naive-rpcid traces against that in rebuild/casper traces\n",
    "    def process_files_for_filled_umdm(self):\n",
    "        naive_rpcid_path = self.dir + 'output-naive-rpcid/'\n",
    "        rebuild_path = self.dir  + 'output-rebuild/'\n",
    "        naive_rpcid_file_names = [file for file in os.listdir(naive_rpcid_path) if file.endswith('.csv.gz')]\n",
    "        rebuild_file_names = [filename.replace('naive-rpcid', 'rebuild') for filename in naive_rpcid_file_names]\n",
    "        size = len(rebuild_file_names)\n",
    "\n",
    "        for i in range(size):\n",
    "            rpcid_file = naive_rpcid_path + naive_rpcid_file_names[i]\n",
    "            rebuild_file = rebuild_path + rebuild_file_names[i]\n",
    "            temp_char = FilledMissValCharacterizer(self.dir, rpcid_file, rebuild_file, self.year)\n",
    "            temp_char.process_file()\n",
    "        helper = FilledMissValHelper(self.dir, year, 'dm') # change \"dm\" to \"um\" to get \"um\" stats\n",
    "        helper.process_all_files()\n",
    "        self.num_filled_dm_list, self.perc_filled_dm_list = helper.process_all_files()\n",
    "\n",
    "    def process_all_files(self):\n",
    "        self.process_files_for_adding_mising_calls() # processing all the error json files\n",
    "        self.process_files_for_filled_umdm()\n",
    "        \n",
    "        directory_path = self.dir + \"output-rebuild/\"\n",
    "        num_files = 0 \n",
    "        \n",
    "        for root, dirs, files in os.walk(directory_path):\n",
    "            \n",
    "            for file in files: # all files has .csv extensions\n",
    "                if file.endswith('csv.gz'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    num_files += 1\n",
    "                    \n",
    "                    # Read the CSV file into a DataFrame\n",
    "                    # Specify the first 9 column names or indices\n",
    "                    columns_to_read = []\n",
    "                    \n",
    "                    if self.year == '2021':\n",
    "                        columns_to_read = ['traceid', 'timestamp', 'rpcid', 'um', 'rpctype', 'dm', 'interface', 'rt']\n",
    "                    else:\n",
    "                        columns_to_read = ['traceid', 'rpcid', 'rpctype','um', 'interface', 'dm',  'rt']\n",
    "                    \n",
    "                    df = pd.read_csv(file_path, compression='gzip', usecols=columns_to_read)\n",
    "                    df['rpcid'] = df['rpcid'].astype(str)\n",
    "                    self.process_one_file(df)\n",
    "                    del df\n",
    "\n",
    "    \n",
    "    # used to sort rpcids\n",
    "    def key(self, rpcid):\n",
    "        return rpcid.count('.')\n",
    "\n",
    "    def sort_rpcids(self, rpcids):\n",
    "        # sort first by # of periods then by last digit \n",
    "        return sorted(rpcids, key=self.key)\n",
    "\n",
    "    \n",
    "    def process_one_file(self, df):\n",
    "        trace_dfs= df.groupby('traceid')\n",
    "\n",
    "        for tid, trace_df in trace_dfs:\n",
    "            \n",
    "            rpcids = trace_df['rpcid'].tolist()\n",
    "            rpcids = self.sort_rpcids(rpcids)\n",
    "            sources_to_nums={}  # store per-source \n",
    "\n",
    "            for rpcid in rpcids:\n",
    "\n",
    "                last_hyphen_index = rpcid.rfind('-')\n",
    "                if last_hyphen_index == -1:\n",
    "                    continue # skip if we didn't modify\n",
    "\n",
    "                source = str (rpcid.split('-', 1)[0])\n",
    "\n",
    "                # a cpe source\n",
    "                if (rpcid.count('-') == 2) and ('.' not in rpcid[last_hyphen_index:]):\n",
    "                    if source not in sources_to_nums:\n",
    "                        sources_to_nums[source] = {\"num_rpcid_modified\" : 1, \"num_downstream_rpcid\" : 0, \"downstream_cpes\" : set()}\n",
    "                    else:\n",
    "                        # could be another rpcid modified for the an exisitng source\n",
    "                        sources_to_nums[source][\"num_rpcid_modified\"] +=1\n",
    "                # a downstream cpe\n",
    "                elif (rpcid.count('-') != 2) and ('.' not in rpcid[last_hyphen_index:]):\n",
    "                    second_last_hyphen_index = rpcid.rfind(\"-\", 0, rpcid.rfind(\"-\"))\n",
    "                    cur_cpe = rpcid[:second_last_hyphen_index]\n",
    "                    sources_to_nums[source][\"downstream_cpes\"].add(cur_cpe)\n",
    "                    sources_to_nums[source][\"num_downstream_rpcid\"] += 1\n",
    "                else: # any other modified downstream rpcid\n",
    "                    sources_to_nums[source][\"num_downstream_rpcid\"] += 1\n",
    "            \n",
    "            # done collecting for the current trace\n",
    "            \n",
    "            # extending global lists, each entry represents info per source of CPE regardless individual traces\n",
    "            self.num_rpcids_at_source_list.extend([source_data[\"num_rpcid_modified\"] for source_data in sources_to_nums.values()]) \n",
    "            self.num_rpcids_downstream_list.extend([source_data[\"num_downstream_rpcid\"] for source_data in sources_to_nums.values()])\n",
    "\n",
    "            downstream_cpes_counts = []\n",
    "            for source_data in sources_to_nums.values():\n",
    "                downstream_cpes_set = source_data[\"downstream_cpes\"]  # Get downstream_cpes set\n",
    "                downstream_cpes_count = len(downstream_cpes_set)  # Count elements in the set\n",
    "                downstream_cpes_counts.append(downstream_cpes_count)  # Append count to list\n",
    "            \n",
    "            self.num_downstream_cpes_list.extend(downstream_cpes_counts)\n",
    "            \n",
    "            self.num_traces_processed += 1\n",
    "\n",
    "    \n",
    "\n",
    "    def print_stats(self, cur_list):\n",
    "        # number of trace affected\n",
    "        num_total = len(cur_list)\n",
    "        \n",
    "        pos_list = [num for num in cur_list if num > 0]\n",
    "        num_affected = len(pos_list)\n",
    "        ratio = num_affected / num_total\n",
    "        percentage = round(ratio * 100, 2)\n",
    "        \n",
    "        print(\"total_traces : \" + (str)(num_total)) \n",
    "        print(\"affected_traces : \" + (str)(num_affected))\n",
    "        print(\"percentage of traces affected: \" + (str) (percentage))\n",
    "       \n",
    "        stats = pd.Series(pos_list).describe().round(2)\n",
    "        print(stats)    \n",
    "        self.print_p99(pos_list)\n",
    "\n",
    "\n",
    "    def print_p99(self, data):\n",
    "        p99 = np.percentile(data, 99)\n",
    "        rounded_p99 = round(p99, 2)\n",
    "        print(\"99th percentile (P99):\", rounded_p99) \n",
    "    \n",
    "    # get the total number of traces in all csv files in a directory\n",
    "    def count_num_trace_id(self, directory_path):\n",
    "        count = 0\n",
    "        for root, dirs, files in os.walk(directory_path):\n",
    "            for file in files:\n",
    "                if file.endswith('.csv.gz'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    df = pd.read_csv(file_path, compression='gzip')\n",
    "                    count += len(df.traceid.unique())\n",
    "                    del df\n",
    "        return count\n",
    "    \n",
    "    def stats_adding_missing_calls(self):\n",
    "        print(\"============== YEAR: \" + self.year + \" ==============\")\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~ adding_missing_calls ~~~~~~~~~~~~~\")\n",
    "        self.print_stats(self.num_miss_rpcid_list)\n",
    "\n",
    "    def stats_filled_in_missing_DMs(self): \n",
    "        print(\"~~~~~~~~~~~~~~~~ ####### num_filled_in_missing_DMs #######~~~~~~~~~~~~~~~~\")\n",
    "        self.print_stats(self.num_filled_dm_list)\n",
    "        print(\"\")\n",
    "        print(\"~~~~~~~~~~~~~~~~ ####### perc_filled_in_missing_DMs #######~~~~~~~~~~~~~~~~\")\n",
    "        self.print_stats(self.perc_filled_dm_list)\n",
    "        print(\"\")\n",
    "    \n",
    "    def stats_updated_rpcids_CPE_source(self):\n",
    "        print(\"~~~~~~~~~~~~~~~~ ####### num_modified_rpcids_at_source_cpe ####### ~~~~~~~~~~~~~~~~\")\n",
    "        print(pd.Series(self.num_rpcids_at_source_list).describe().round(2)) \n",
    "        self.print_p99(self.num_rpcids_at_source_list)\n",
    "        print(\"\")\n",
    "\n",
    "    def stats_fixed_downstream_CPEs(self):\n",
    "        print(\"~~~~~~~~~~~~~~~~ ####### num_downstream_CPEs ####### ~~~~~~~~~~~~~~~~\")\n",
    "        print(pd.Series(self.num_downstream_cpes_list).describe().round(2)) \n",
    "        self.print_p99(self.num_downstream_cpes_list)\n",
    "        print(\"\")\n",
    "\n",
    "    def stats_recovered_rpcids_CPE_downstream(self):\n",
    "        print(\"~~~~~~~~~~~~~~~~ ####### num_modified_downstream_rpcid ####### ~~~~~~~~~~~~~~~~\")\n",
    "        print(pd.Series(self.num_rpcids_downstream_list).describe().round(2)) \n",
    "        self.print_p99(self.num_rpcids_downstream_list)\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "\n",
    "    def stats_additional_complete_traces(self):\n",
    "        num_naive_rpcid = self.count_num_trace_id(self.dir + \"output-naive-rpcid/\")\n",
    "        num_naive_accurate = self.count_num_trace_id(self.dir + \"output-naive-accurate/\")\n",
    "        \n",
    "        print(\"============== YEAR: \" + self.year + \" ==============\")\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~ COMPLETE TRACES ~~~~~~~~~~~~~\")\n",
    "        # number of trace affected\n",
    "        num_total = len(self.num_unrecoverable_rpcid_list)\n",
    "        zero_list = [num for num in self.num_unrecoverable_rpcid_list if num == 0]\n",
    "        num_zero = len(zero_list)\n",
    "        ratio = num_zero / num_total\n",
    "        percentage = round(ratio * 100, 2)\n",
    "        print(\"num_total_rebuild_traces : \" + (str)(num_total)) \n",
    "        print(\"num_complete_rebuild_traces : \" + (str)(num_zero))\n",
    "        print(\"percentage of complete traces: \" + (str) (percentage))\n",
    "\n",
    "        print(\"~~~~~~~~~~~~~~~~~~~~ ADDITIONAL COMPLETE TRACES ~~~~~~~~~~~~~\")\n",
    "        print(\"num_total_traces, i.e. num_naive_rpcid_traces: \" + (str)(num_naive_rpcid) )    \n",
    "        print(\"num_naive_accurate_traces: \" + (str)(num_naive_accurate) ) \n",
    "        print(\"num_complete_rebuild_traces : \" + (str)(num_zero))\n",
    "        num_additional_complete = num_zero - num_naive_accurate\n",
    "        print(\"num_additional_complete_traces: \" + (str)(num_additional_complete))\n",
    "        ratio_2 = num_additional_complete / num_naive_rpcid\n",
    "        percentage_2 = round(ratio_2 * 100, 2)\n",
    "        print(\"perc_additional_complete_traces: \"  + (str)(percentage_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d538c686-fce0-4f24-8a03-4b11e5658934",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_char = ImpactCharacterizer(path, year)\n",
    "impact_char.process_all_files() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29929f6-69c7-4f49-b725-36681ff4bc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_char.stats_adding_missing_calls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9cbae2-e65f-4c76-800d-3b529bd1d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_char.stats_filled_in_missing_DMs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e89909e-427e-4ad0-8d51-c004bdd02c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_char.stats_updated_rpcids_CPE_source()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369e602f-2523-40a4-b93d-facd04cc0fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_char.stats_recovered_rpcids_CPE_downstream()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2459a1-76a1-412c-a3b7-8981389bfac0",
   "metadata": {},
   "source": [
    "## Additional complete traces\n",
    "A trace is complete if there are no unrecoverable rpcids (explained in the paper Section 4.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53852f43-d3aa-4013-bf79-bf747efea950",
   "metadata": {},
   "outputs": [],
   "source": [
    "impact_char.stats_additional_complete_traces()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
